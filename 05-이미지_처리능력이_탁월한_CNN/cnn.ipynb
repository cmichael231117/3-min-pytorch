{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#일반적인 인공 신경망은 다양한 형태의 입력에 대한 확장성이 떨어진다. -> 컨볼루션 :필터를 적용. 이 필터를 학습\n",
    "#컨볼루션 레이어 : 이미지의 특징을 추출\n",
    "#풀링 레이어 : 중요한 특징을 골라낸다. 일종의 컨볼루션\n",
    "#피처 맵의 크기가 크면 학습이 어렵고 오버피팅의 위험이 증가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN으로 패션 아이템 구분하기\n",
    "Convolutional Neural Network (CNN) 을 이용하여 패션아이템 구분 성능을 높여보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy 전체 중에 true인 분류의 비율\n",
    "#recall 암환자를 한명도 놓치고 싶지 않다. 웬만하면 암이라 분류 (실제 positive가 분모:FN+TP)\n",
    "#precision 나는 정확도 높은 판단을 하고 싶다. 보수적으로 판단. 확실한 경우에만 암이라 분류 (내가 positive라 예측한 것이 분모:FP+TP)\n",
    "#ROC 커브 이진분류기에서 많이 사용. feature 선택에 유용 [y축:TP/(TP+FN)실제 암인데 맞춤recall], [x축:FP/(TN+FP)정상인데 암이라함]\n",
    "#AUC(Area Under the Curve) : ROC커브의 면적. 클수록 decision boundary에 덜 민감\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS     = 40\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "9<00:41, 434519.71it/s]\u001b[A\n 32%|███▏      | 8560640/26421880 [00:20<00:37, 480458.12it/s]\u001b[A\n 33%|███▎      | 8675328/26421880 [00:20<00:41, 424984.07it/s]\u001b[A\n 34%|███▎      | 8863744/26421880 [00:20<00:40, 438048.94it/s]\u001b[A\n 34%|███▍      | 8970240/26421880 [00:21<00:42, 410334.71it/s]\u001b[A\n 34%|███▍      | 9093120/26421880 [00:21<00:42, 404376.09it/s]\u001b[A\n 35%|███▍      | 9216000/26421880 [00:21<00:42, 403710.77it/s]\u001b[A\n 35%|███▌      | 9347072/26421880 [00:22<00:51, 333660.71it/s]\u001b[A\n 36%|███▌      | 9560064/26421880 [00:22<00:37, 444035.03it/s]\u001b[A\n 37%|███▋      | 9650176/26421880 [00:22<00:35, 478606.83it/s]\u001b[A\n 37%|███▋      | 9732096/26421880 [00:23<00:49, 333834.85it/s]\u001b[A\n 37%|███▋      | 9797632/26421880 [00:23<00:57, 286951.62it/s]\u001b[A\n 37%|███▋      | 9863168/26421880 [00:23<01:03, 261239.99it/s]\u001b[A\n 38%|███▊      | 9945088/26421880 [00:24<01:40, 164635.63it/s]\u001b[A\n 38%|███▊      | 10158080/26421880 [00:24<01:16, 213952.09it/s]\u001b[A\n 39%|███▉      | 10240000/26421880 [00:25<01:11, 226672.14it/s]\u001b[A\n 39%|███▉      | 10321920/26421880 [00:25<01:07, 237583.21it/s]\u001b[A\n 39%|███▉      | 10403840/26421880 [00:25<01:05, 244691.91it/s]\u001b[A\n 40%|███▉      | 10493952/26421880 [00:26<01:01, 257836.48it/s]\u001b[A\n 40%|████      | 10584064/26421880 [00:26<00:57, 273591.62it/s]\u001b[A\n 40%|████      | 10674176/26421880 [00:26<00:53, 295401.30it/s]\u001b[A\n 41%|████      | 10764288/26421880 [00:27<00:57, 270885.79it/s]\u001b[A\n 41%|████      | 10854400/26421880 [00:27<00:55, 279745.91it/s]\u001b[A\n 41%|████▏     | 10944512/26421880 [00:27<00:54, 283976.70it/s]\u001b[A\n 42%|████▏     | 11042816/26421880 [00:27<00:52, 292851.38it/s]\u001b[A\n 42%|████▏     | 11132928/26421880 [00:28<00:52, 293503.99it/s]\u001b[A\n 42%|████▏     | 11223040/26421880 [00:28<00:51, 294108.79it/s]\u001b[A\n 43%|████▎     | 11313152/26421880 [00:28<00:51, 293701.59it/s]\u001b[A\n 43%|████▎     | 11403264/26421880 [00:29<00:51, 293697.25it/s]\u001b[A\n 44%|████▎     | 11501568/26421880 [00:29<00:49, 300661.41it/s]\u001b[A\n 44%|████▍     | 11591680/26421880 [00:29<00:49, 298962.19it/s]\u001b[A\n 44%|████▍     | 11689984/26421880 [00:30<00:48, 302542.36it/s]\u001b[A\n 45%|████▍     | 11788288/26421880 [00:30<00:44, 326042.43it/s]\u001b[A\n 45%|████▍     | 11886592/26421880 [00:30<00:47, 308140.56it/s]\u001b[A\n 45%|████▌     | 11993088/26421880 [00:31<00:43, 328242.77it/s]\u001b[A\n 46%|████▌     | 12107776/26421880 [00:31<00:43, 327339.11it/s]\u001b[A\n 46%|████▋     | 12222464/26421880 [00:31<00:41, 342353.34it/s]\u001b[A\n 47%|████▋     | 12353536/26421880 [00:32<00:48, 290837.52it/s]\u001b[A\n 47%|████▋     | 12394496/26421880 [00:32<01:06, 210128.81it/s]\u001b[A\n 47%|████▋     | 12427264/26421880 [00:32<01:23, 166720.69it/s]\u001b[A\n 48%|████▊     | 12615680/26421880 [00:33<01:04, 213107.82it/s]\u001b[A\n 48%|████▊     | 12722176/26421880 [00:33<00:54, 250857.61it/s]\u001b[A\n 49%|████▊     | 12828672/26421880 [00:33<00:47, 287862.64it/s]\u001b[A\n 49%|████▉     | 12951552/26421880 [00:33<00:42, 314596.88it/s]\u001b[A\n 49%|████▉     | 13033472/26421880 [00:34<00:53, 249302.06it/s]\u001b[A\n 50%|████▉     | 13205504/26421880 [00:34<00:42, 308423.95it/s]\u001b[A\n 50%|█████     | 13295616/26421880 [00:34<00:40, 324404.46it/s]\u001b[A\n 51%|█████     | 13393920/26421880 [00:35<00:38, 342396.52it/s]\u001b[A\n 51%|█████     | 13492224/26421880 [00:35<00:33, 381538.18it/s]\u001b[A\n 51%|█████▏    | 13574144/26421880 [00:35<00:28, 454323.93it/s]\u001b[A\n 52%|█████▏    | 13631488/26421880 [00:35<00:49, 256504.37it/s]\u001b[A\n 52%|█████▏    | 13697024/26421880 [00:36<00:52, 243458.29it/s]\u001b[A\n 52%|█████▏    | 13803520/26421880 [00:36<00:47, 267259.85it/s]\u001b[A\n 53%|█████▎    | 13910016/26421880 [00:36<00:42, 295152.73it/s]\u001b[A\n 53%|█████▎    | 13991936/26421880 [00:37<00:45, 270460.65it/s]\u001b[A\n 53%|█████▎    | 14098432/26421880 [00:37<00:42, 293278.67it/s]\u001b[A\n 54%|█████▎    | 14172160/26421880 [00:37<00:41, 295503.55it/s]\u001b[A\n 54%|█████▍    | 14262272/26421880 [00:37<00:38, 313704.97it/s]\u001b[A\n 54%|█████▍    | 14352384/26421880 [00:38<00:44, 274132.09it/s]\u001b[A\n 55%|█████▍    | 14442496/26421880 [00:38<00:40, 296986.06it/s]\u001b[A\n 55%|█████▌    | 14548992/26421880 [00:39<00:40, 293435.54it/s]\u001b[A\n 55%|█████▌    | 14647296/26421880 [00:39<00:39, 299831.06it/s]\u001b[A\n 56%|█████▌    | 14753792/26421880 [00:39<00:37, 313294.16it/s]\u001b[A\n 56%|█████▌    | 14860288/26421880 [00:39<00:34, 333643.31it/s]\u001b[A\n 57%|█████▋    | 14966784/26421880 [00:40<00:31, 358469.72it/s]\u001b[A\n 57%|█████▋    | 15073280/26421880 [00:40<00:29, 378457.24it/s]\u001b[A\n 57%|█████▋    | 15187968/26421880 [00:40<00:34, 326419.98it/s]\u001b[A\n 58%|█████▊    | 15294464/26421880 [00:41<00:33, 332030.17it/s]\u001b[A\n 58%|█████▊    | 15400960/26421880 [00:41<00:32, 336847.21it/s]\u001b[A\n 59%|█████▊    | 15507456/26421880 [00:42<00:41, 260961.52it/s]\u001b[A\n 59%|█████▉    | 15622144/26421880 [00:42<00:37, 288290.88it/s]\u001b[A\n 60%|█████▉    | 15728640/26421880 [00:43<00:44, 240298.62it/s]\u001b[A\n 60%|█████▉    | 15835136/26421880 [00:43<00:39, 264796.64it/s]\u001b[A\n 60%|██████    | 15949824/26421880 [00:43<00:36, 285160.12it/s]\u001b[A\n 61%|██████    | 16039936/26421880 [00:43<00:35, 288923.37it/s]\u001b[A\n 61%|██████    | 16072704/26421880 [00:44<00:52, 197702.88it/s]\u001b[A\n 61%|██████▏   | 16244736/26421880 [00:44<00:40, 249429.00it/s]\u001b[A\n 62%|██████▏   | 16334848/26421880 [00:44<00:37, 272370.72it/s]\u001b[A\n 62%|██████▏   | 16424960/26421880 [00:45<00:33, 295901.25it/s]\u001b[A\n 63%|██████▎   | 16523264/26421880 [00:45<00:37, 265950.58it/s]\u001b[A\n 63%|██████▎   | 16629760/26421880 [00:45<00:34, 284839.26it/s]\u001b[A\n 63%|██████▎   | 16744448/26421880 [00:46<00:31, 306581.48it/s]\u001b[A\n 64%|██████▎   | 16785408/26421880 [00:46<00:43, 221462.80it/s]\u001b[A\n 64%|██████▎   | 16834560/26421880 [00:46<00:44, 215736.00it/s]\u001b[A\n 64%|██████▍   | 16916480/26421880 [00:46<00:39, 240313.56it/s]\u001b[A\n 64%|██████▍   | 16998400/26421880 [00:47<00:42, 222345.96it/s]\u001b[A\n 65%|██████▍   | 17080320/26421880 [00:47<00:40, 231779.68it/s]\u001b[A\n 65%|██████▍   | 17170432/26421880 [00:47<00:35, 260187.50it/s]\u001b[A\n 65%|██████▌   | 17268736/26421880 [00:48<00:34, 265800.60it/s]\u001b[A\n 66%|██████▌   | 17358848/26421880 [00:48<00:33, 273086.96it/s]\u001b[A\n 66%|██████▌   | 17440768/26421880 [00:49<00:43, 208197.10it/s]\u001b[A\n 67%|██████▋   | 17588224/26421880 [00:49<00:35, 250771.90it/s]\u001b[A\n 67%|██████▋   | 17661952/26421880 [00:49<00:35, 247118.77it/s]\u001b[A\n 67%|██████▋   | 17743872/26421880 [00:50<00:32, 267881.72it/s]\u001b[A\n 67%|██████▋   | 17825792/26421880 [00:50<00:34, 252441.49it/s]\u001b[A\n 68%|██████▊   | 17907712/26421880 [00:50<00:33, 254177.32it/s]\u001b[A\n 68%|██████▊   | 18006016/26421880 [00:50<00:29, 282352.96it/s]\u001b[A\n 68%|██████▊   | 18096128/26421880 [00:51<00:30, 275182.36it/s]\u001b[A\n 69%|██████▉   | 18194432/26421880 [00:51<00:36, 226017.73it/s]\u001b[A\n 69%|██████▉   | 18284544/26421880 [00:52<00:33, 242742.02it/s]\u001b[A\n 70%|██████▉   | 18382848/26421880 [00:52<00:29, 274823.19it/s]\u001b[A\n 70%|██████▉   | 18481152/26421880 [00:52<00:26, 303743.71it/s]\u001b[A\n 70%|███████   | 18579456/26421880 [00:52<00:23, 327579.09it/s]\u001b[A\n 71%|███████   | 18677760/26421880 [00:53<00:35, 217858.63it/s]\u001b[A\n 71%|███████   | 18767872/26421880 [00:53<00:27, 279309.89it/s]\u001b[A\n 71%|███████   | 18817024/26421880 [00:54<00:26, 285450.46it/s]\u001b[A\n 71%|███████▏  | 18866176/26421880 [00:54<00:26, 280277.71it/s]\u001b[A\n 72%|███████▏  | 18964480/26421880 [00:54<00:29, 256554.13it/s]\u001b[A\n 72%|███████▏  | 19070976/26421880 [00:54<00:25, 290392.54it/s]\u001b[A\n 73%|███████▎  | 19161088/26421880 [00:55<00:19, 363328.82it/s]\u001b[A\n 73%|███████▎  | 19218432/26421880 [00:55<00:23, 301543.11it/s]\u001b[A\n 73%|███████▎  | 19267584/26421880 [00:55<00:26, 265231.24it/s]\u001b[A\n 73%|███████▎  | 19374080/26421880 [00:55<00:25, 271430.33it/s]\u001b[A\n 74%|███████▍  | 19488768/26421880 [00:56<00:29, 235719.59it/s]\u001b[A\n 74%|███████▍  | 19554304/26421880 [00:57<00:39, 174932.49it/s]\u001b[A\n 74%|███████▍  | 19595264/26421880 [00:57<00:42, 159079.88it/s]\u001b[A\n 74%|███████▍  | 19619840/26421880 [00:57<00:50, 135717.87it/s]\u001b[A\n 75%|███████▍  | 19791872/26421880 [00:58<00:38, 171172.17it/s]\u001b[A\n 75%|███████▌  | 19881984/26421880 [00:58<00:39, 165952.08it/s]\u001b[A\n 76%|███████▌  | 19972096/26421880 [00:58<00:33, 191134.79it/s]\u001b[A\n 76%|███████▌  | 20070400/26421880 [00:59<00:40, 156263.28it/s]\u001b[A\n 76%|███████▋  | 20185088/26421880 [01:00<00:33, 185680.37it/s]\u001b[A\n 77%|███████▋  | 20299776/26421880 [01:00<00:27, 220536.59it/s]\u001b[A\n 77%|███████▋  | 20422656/26421880 [01:00<00:23, 254699.67it/s]\u001b[A\n 78%|███████▊  | 20545536/26421880 [01:01<00:20, 291263.44it/s]\u001b[A\n 78%|███████▊  | 20668416/26421880 [01:01<00:17, 332130.84it/s]\u001b[A\n 79%|███████▊  | 20791296/26421880 [01:01<00:15, 368788.00it/s]\u001b[A\n 79%|███████▉  | 20914176/26421880 [01:02<00:16, 331559.88it/s]\u001b[A\n 79%|███████▉  | 20955136/26421880 [01:02<00:23, 227961.11it/s]\u001b[A\n 80%|███████▉  | 21094400/26421880 [01:02<00:19, 269073.06it/s]\u001b[A\n 80%|████████  | 21184512/26421880 [01:03<00:24, 215183.28it/s]\u001b[A\n 81%|████████  | 21282816/26421880 [01:03<00:21, 238004.89it/s]\u001b[A\n 81%|████████  | 21348352/26421880 [01:03<00:22, 229427.41it/s]\u001b[A\n 81%|████████  | 21381120/26421880 [01:04<00:29, 169345.61it/s]\u001b[A\n 82%|████████▏ | 21544960/26421880 [01:04<00:22, 214046.61it/s]\u001b[A\n 82%|████████▏ | 21626880/26421880 [01:04<00:21, 227005.39it/s]\u001b[A\n 82%|████████▏ | 21708800/26421880 [01:05<00:19, 238211.37it/s]\u001b[A\n 83%|████████▎ | 21798912/26421880 [01:05<00:18, 251763.16it/s]\u001b[A\n 83%|████████▎ | 21889024/26421880 [01:05<00:17, 261986.41it/s]\u001b[A\n 83%|████████▎ | 21979136/26421880 [01:06<00:16, 271338.53it/s]\u001b[A\n 84%|████████▎ | 22069248/26421880 [01:06<00:15, 278294.31it/s]\u001b[A\n 84%|████████▍ | 22167552/26421880 [01:06<00:14, 288815.31it/s]\u001b[A\n 84%|████████▍ | 22257664/26421880 [01:06<00:14, 290627.47it/s]\u001b[A\n 85%|████████▍ | 22347776/26421880 [01:07<00:13, 311003.51it/s]\u001b[A\n 85%|████████▍ | 22446080/26421880 [01:07<00:17, 228865.15it/s]\u001b[A\n 85%|████████▌ | 22536192/26421880 [01:08<00:15, 248570.34it/s]\u001b[A\n 86%|████████▌ | 22634496/26421880 [01:08<00:13, 279073.77it/s]\u001b[A\n 86%|████████▌ | 22724608/26421880 [01:08<00:13, 267214.27it/s]\u001b[A\n 86%|████████▌ | 22757376/26421880 [01:09<00:20, 180445.62it/s]\u001b[A\n 87%|████████▋ | 22888448/26421880 [01:09<00:15, 223719.53it/s]\u001b[A\n 87%|████████▋ | 22962176/26421880 [01:09<00:14, 230995.75it/s]\u001b[A\n 87%|████████▋ | 23035904/26421880 [01:10<00:15, 223097.94it/s]\u001b[A\n 87%|████████▋ | 23117824/26421880 [01:10<00:13, 243881.87it/s]\u001b[A\n 88%|████████▊ | 23199744/26421880 [01:10<00:13, 247842.84it/s]\u001b[A\n 88%|████████▊ | 23289856/26421880 [01:10<00:10, 289924.89it/s]\u001b[A\n 88%|████████▊ | 23379968/26421880 [01:11<00:10, 282905.61it/s]\u001b[A\n 89%|████████▉ | 23478272/26421880 [01:11<00:09, 299964.98it/s]\u001b[A\n 89%|████████▉ | 23511040/26421880 [01:11<00:09, 307457.43it/s]\u001b[A\n 89%|████████▉ | 23568384/26421880 [01:11<00:08, 341826.60it/s]\u001b[A\n 89%|████████▉ | 23609344/26421880 [01:11<00:08, 326737.95it/s]\u001b[A\n 90%|████████▉ | 23658496/26421880 [01:11<00:08, 339891.72it/s]\u001b[A\n 90%|████████▉ | 23699456/26421880 [01:12<00:08, 336975.10it/s]\u001b[A\n 90%|████████▉ | 23748608/26421880 [01:12<00:07, 369288.81it/s]\u001b[A\n 90%|█████████ | 23789568/26421880 [01:12<00:07, 351410.29it/s]\u001b[A\n 90%|█████████ | 23838720/26421880 [01:12<00:06, 371085.19it/s]\u001b[A\n 90%|█████████ | 23879680/26421880 [01:12<00:07, 330197.59it/s]\u001b[A\n 91%|█████████ | 23937024/26421880 [01:12<00:09, 270000.57it/s]\u001b[A\n 91%|█████████ | 24035328/26421880 [01:13<00:07, 299307.47it/s]\u001b[A\n 91%|█████████▏| 24125440/26421880 [01:13<00:07, 318105.86it/s]\u001b[A\n 92%|█████████▏| 24223744/26421880 [01:13<00:06, 336415.67it/s]\u001b[A\n 92%|█████████▏| 24322048/26421880 [01:13<00:05, 351502.98it/s]\u001b[A\n 92%|█████████▏| 24412160/26421880 [01:14<00:05, 358172.78it/s]\u001b[A\n 93%|█████████▎| 24510464/26421880 [01:14<00:05, 367903.63it/s]\u001b[A\n 93%|█████████▎| 24608768/26421880 [01:14<00:04, 376710.14it/s]\u001b[A\n 94%|█████████▎| 24707072/26421880 [01:14<00:04, 384053.43it/s]\u001b[A\n 94%|█████████▍| 24805376/26421880 [01:15<00:04, 388280.83it/s]\u001b[A\n 94%|█████████▍| 24903680/26421880 [01:15<00:03, 441735.94it/s]\u001b[A\n 94%|█████████▍| 24952832/26421880 [01:15<00:03, 412838.67it/s]\u001b[A\n 95%|█████████▍| 25010176/26421880 [01:15<00:03, 362046.82it/s]\u001b[A\n 95%|█████████▌| 25116672/26421880 [01:15<00:03, 378619.26it/s]\u001b[A\n 95%|█████████▌| 25223168/26421880 [01:15<00:02, 445867.63it/s]\u001b[A\n 96%|█████████▌| 25280512/26421880 [01:16<00:02, 401583.68it/s]\u001b[A\n 96%|█████████▌| 25337856/26421880 [01:16<00:02, 381404.24it/s]\u001b[A\n 96%|█████████▋| 25436160/26421880 [01:16<00:02, 428324.88it/s]\u001b[A\n 96%|█████████▋| 25485312/26421880 [01:16<00:02, 382449.64it/s]\u001b[A\n 97%|█████████▋| 25575424/26421880 [01:16<00:02, 417159.42it/s]\u001b[A\n 97%|█████████▋| 25681920/26421880 [01:16<00:01, 471122.49it/s]\u001b[A\n 97%|█████████▋| 25739264/26421880 [01:17<00:01, 403369.17it/s]\u001b[A\n 98%|█████████▊| 25837568/26421880 [01:17<00:01, 461758.15it/s]\u001b[A\n 98%|█████████▊| 25894912/26421880 [01:17<00:01, 477650.62it/s]\u001b[A\n 98%|█████████▊| 25976832/26421880 [01:17<00:00, 509793.44it/s]\u001b[A\n 99%|█████████▊| 26034176/26421880 [01:17<00:00, 514491.41it/s]\u001b[A\n 99%|█████████▉| 26116096/26421880 [01:17<00:00, 526621.67it/s]\u001b[A\n 99%|█████████▉| 26206208/26421880 [01:17<00:00, 599592.27it/s]\u001b[A\n 99%|█████████▉| 26271744/26421880 [01:18<00:00, 548392.00it/s]\u001b[A\n100%|█████████▉| 26361856/26421880 [01:18<00:00, 620248.29it/s]\u001b[AExtracting ./.data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./.data/FashionMNIST/raw\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[ADownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./.data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n\n\n  0%|          | 0/29515 [00:00<?, ?it/s]\u001b[A\u001b[A\n\n32768it [00:01, 31239.42it/s]\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[AExtracting ./.data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./.data/FashionMNIST/raw\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./.data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n\n\n  0%|          | 0/4422102 [00:00<?, ?it/s]\u001b[A\u001b[A\n\n  0%|          | 16384/4422102 [00:00<01:23, 52988.68it/s]\u001b[A\u001b[A\n\n  1%|          | 49152/4422102 [00:01<01:07, 64492.27it/s]\u001b[A\u001b[A\n\n  2%|▏         | 98304/4422102 [00:01<00:56, 76370.72it/s]\u001b[A\u001b[A\n\n  2%|▏         | 106496/4422102 [00:01<01:27, 49435.32it/s]\u001b[A\u001b[A\n\n  4%|▍         | 188416/4422102 [00:02<01:03, 66363.50it/s]\u001b[A\u001b[A\n\n  5%|▌         | 229376/4422102 [00:02<00:55, 75241.28it/s]\u001b[A\u001b[A\n\n  6%|▌         | 270336/4422102 [00:02<00:46, 89005.38it/s]\u001b[A\u001b[A\n\n  7%|▋         | 319488/4422102 [00:02<00:38, 107138.53it/s]\u001b[A\u001b[A\n\n  9%|▊         | 376832/4422102 [00:03<00:35, 115115.79it/s]\u001b[A\u001b[A\n\n 10%|▉         | 425984/4422102 [00:03<00:30, 130095.35it/s]\u001b[A\u001b[A\n\n 11%|█         | 483328/4422102 [00:03<00:28, 137662.76it/s]\u001b[A\u001b[A\n\n 12%|█▏        | 540672/4422102 [00:04<00:24, 156423.11it/s]\u001b[A\u001b[A\n\n 14%|█▎        | 598016/4422102 [00:04<00:22, 170736.28it/s]\u001b[A\u001b[A\n\n 15%|█▍        | 647168/4422102 [00:04<00:20, 188659.47it/s]\u001b[A\u001b[A\n\n 15%|█▌        | 671744/4422102 [00:05<00:32, 115484.21it/s]\u001b[A\u001b[A\n\n 16%|█▌        | 696320/4422102 [00:05<00:49, 74974.46it/s]\u001b[A\u001b[A\n\n 16%|█▋        | 729088/4422102 [00:06<00:48, 75687.23it/s]\u001b[A\u001b[A\n\n 17%|█▋        | 745472/4422102 [00:06<00:54, 67115.71it/s]\u001b[A\u001b[A\n\n 18%|█▊        | 786432/4422102 [00:06<00:46, 78795.23it/s]\u001b[A\u001b[A\n\n 18%|█▊        | 811008/4422102 [00:06<00:43, 83330.30it/s]\u001b[A\u001b[A\n\n 19%|█▊        | 827392/4422102 [00:07<00:53, 66836.65it/s]\u001b[A\u001b[A\n\n 19%|█▉        | 843776/4422102 [00:07<00:57, 62234.61it/s]\u001b[A\u001b[A\n\n 20%|█▉        | 868352/4422102 [00:07<00:53, 66641.96it/s]\u001b[A\u001b[A\n\n 20%|██        | 892928/4422102 [00:08<00:47, 74034.44it/s]\u001b[A\u001b[A\n\n 21%|██        | 909312/4422102 [00:08<00:56, 61772.30it/s]\u001b[A\u001b[A\n\n 21%|██        | 933888/4422102 [00:08<00:52, 66275.35it/s]\u001b[A\u001b[A\n\n 21%|██▏       | 950272/4422102 [00:09<00:52, 65994.25it/s]\u001b[A\u001b[A\n\n 22%|██▏       | 974848/4422102 [00:09<00:47, 72886.51it/s]\u001b[A\u001b[A\n\n 23%|██▎       | 999424/4422102 [00:09<00:50, 68056.29it/s]\u001b[A\u001b[A\n\n 23%|██▎       | 1024000/4422102 [00:10<00:47, 71319.39it/s]\u001b[A\u001b[A\n\n 24%|██▎       | 1040384/4422102 [00:10<00:52, 64677.39it/s]\u001b[A\u001b[A\n\n 24%|██▍       | 1064960/4422102 [00:10<00:48, 68726.70it/s]\u001b[A\u001b[A\n\n 25%|██▍       | 1097728/4422102 [00:11<00:43, 76731.06it/s]\u001b[A\u001b[A\n\n 25%|██▌       | 1122304/4422102 [00:11<00:42, 77654.99it/s]\u001b[A\u001b[A\n\n 26%|██▌       | 1155072/4422102 [00:11<00:39, 83660.16it/s]\u001b[A\u001b[A\n\n 27%|██▋       | 1187840/4422102 [00:11<00:35, 90611.93it/s]\u001b[A\u001b[A\n\n 28%|██▊       | 1228800/4422102 [00:12<00:31, 99850.37it/s]\u001b[A\u001b[A\n\n 29%|██▊       | 1269760/4422102 [00:12<00:29, 108672.82it/s]\u001b[A\u001b[A\n\n 30%|███       | 1327104/4422102 [00:12<00:24, 124134.84it/s]\u001b[A\u001b[A\n\n 31%|███▏      | 1384448/4422102 [00:13<00:22, 137812.25it/s]\u001b[A\u001b[A\n\n 33%|███▎      | 1458176/4422102 [00:13<00:18, 158144.37it/s]\u001b[A\u001b[A\n\n 35%|███▍      | 1531904/4422102 [00:13<00:16, 175894.97it/s]\u001b[A\u001b[A\n\n 37%|███▋      | 1630208/4422102 [00:14<00:13, 210362.67it/s]\u001b[A\u001b[A\n\n 38%|███▊      | 1662976/4422102 [00:14<00:16, 170820.33it/s]\u001b[A\u001b[A\n\n 38%|███▊      | 1687552/4422102 [00:15<00:35, 77732.47it/s]\u001b[A\u001b[A\n\n 41%|████      | 1818624/4422102 [00:15<00:26, 96610.91it/s]\u001b[A\u001b[A\n\n 44%|████▎     | 1933312/4422102 [00:15<00:20, 123883.75it/s]\u001b[A\u001b[A\n\n 45%|████▌     | 1990656/4422102 [00:16<00:17, 138224.37it/s]\u001b[A\u001b[A\n\n 46%|████▋     | 2056192/4422102 [00:16<00:15, 154306.90it/s]\u001b[A\u001b[A\n\n 48%|████▊     | 2121728/4422102 [00:16<00:13, 168421.16it/s]\u001b[A\u001b[A\n26427392it [01:37, 620248.29it/s]\u001b[A\n\n 50%|████▉     | 2195456/4422102 [00:17<00:12, 184671.52it/s]\u001b[A\u001b[A\n\n 51%|█████▏    | 2269184/4422102 [00:17<00:10, 197855.99it/s]\u001b[A\u001b[A\n\n 53%|█████▎    | 2342912/4422102 [00:17<00:09, 209985.58it/s]\u001b[A\u001b[A\n\n 55%|█████▍    | 2416640/4422102 [00:18<00:09, 218282.74it/s]\u001b[A\u001b[A\n\n 57%|█████▋    | 2498560/4422102 [00:18<00:08, 230250.70it/s]\u001b[A\u001b[A\n\n 58%|█████▊    | 2572288/4422102 [00:18<00:07, 244542.93it/s]\u001b[A\u001b[A\n\n 59%|█████▉    | 2605056/4422102 [00:18<00:09, 193553.32it/s]\u001b[A\u001b[A\n\n 59%|█████▉    | 2629632/4422102 [00:19<00:11, 150818.12it/s]\u001b[A\u001b[A\n\n 61%|██████    | 2695168/4422102 [00:19<00:11, 146952.66it/s]\u001b[A\u001b[A\n\n 62%|██████▏   | 2744320/4422102 [00:19<00:11, 150811.53it/s]\u001b[A\u001b[A\n\n 64%|██████▎   | 2809856/4422102 [00:20<00:09, 164169.74it/s]\u001b[A\u001b[A\n\n 65%|██████▌   | 2875392/4422102 [00:20<00:08, 185270.65it/s]\u001b[A\u001b[A\n\n 67%|██████▋   | 2949120/4422102 [00:20<00:07, 209414.88it/s]\u001b[A\u001b[A\n\n 68%|██████▊   | 3022848/4422102 [00:20<00:06, 230325.82it/s]\u001b[A\u001b[A\n\n 70%|███████   | 3104768/4422102 [00:21<00:06, 208333.05it/s]\u001b[A\u001b[A\n\n 72%|███████▏  | 3178496/4422102 [00:21<00:05, 216965.05it/s]\u001b[A\u001b[A\n\n 74%|███████▎  | 3252224/4422102 [00:22<00:05, 223258.44it/s]\u001b[A\u001b[A\n\n 75%|███████▌  | 3334144/4422102 [00:22<00:04, 234966.84it/s]\u001b[A\u001b[A\n\n 77%|███████▋  | 3407872/4422102 [00:22<00:04, 251274.88it/s]\u001b[A\u001b[A\n\n 79%|███████▉  | 3489792/4422102 [00:22<00:03, 270197.90it/s]\u001b[A\u001b[A\n\n 80%|████████  | 3555328/4422102 [00:22<00:02, 323897.81it/s]\u001b[A\u001b[A\n\n 81%|████████▏ | 3596288/4422102 [00:23<00:03, 251105.70it/s]\u001b[A\u001b[A\n\n 82%|████████▏ | 3637248/4422102 [00:23<00:03, 220438.21it/s]\u001b[A\u001b[A\n\n 83%|████████▎ | 3678208/4422102 [00:23<00:04, 154914.10it/s]\u001b[A\u001b[A\n\n 84%|████████▎ | 3702784/4422102 [00:24<00:05, 120881.50it/s]\u001b[A\u001b[A\n\n 85%|████████▌ | 3768320/4422102 [00:24<00:04, 139362.52it/s]\u001b[A\u001b[A\n\n 87%|████████▋ | 3825664/4422102 [00:24<00:03, 150669.44it/s]\u001b[A\u001b[A\n\n 88%|████████▊ | 3883008/4422102 [00:25<00:03, 159605.75it/s]\u001b[A\u001b[A\n\n 89%|████████▉ | 3956736/4422102 [00:25<00:02, 177063.61it/s]\u001b[A\u001b[A\n\n 91%|█████████ | 4022272/4422102 [00:25<00:02, 187075.50it/s]\u001b[A\u001b[A\n\n 93%|█████████▎| 4096000/4422102 [00:26<00:01, 200592.08it/s]\u001b[A\u001b[A\n\n 94%|█████████▍| 4169728/4422102 [00:26<00:01, 211124.39it/s]\u001b[A\u001b[A\n\n 96%|█████████▌| 4243456/4422102 [00:26<00:00, 219214.04it/s]\u001b[A\u001b[A\n\n 98%|█████████▊| 4325376/4422102 [00:26<00:00, 242508.29it/s]\u001b[A\u001b[A\n\n 99%|█████████▉| 4399104/4422102 [00:27<00:00, 223529.26it/s]\u001b[A\u001b[A\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[AExtracting ./.data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./.data/FashionMNIST/raw\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./.data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n\n\n\n8192it [00:00, 10308.42it/s]\nExtracting ./.data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./.data/FashionMNIST/raw\nProcessing...\nDone!\n"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST('./.data',\n",
    "                   train=True,\n",
    "                   download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST('./.data',\n",
    "                   train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뉴럴넷으로 Fashion MNIST 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self): #학습모듈 정의\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) #nn.Conv2d():입력 x를 받는 함수를 반환. 함수로 취급해도 무방. 1:입력채널, 10:출력채널\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5) #필터사이즈는 (3,5)와 같은 배열을 입력해서 직사각형으로 만들수도 있음\n",
    "        self.conv2_drop = nn.Dropout2d() #함수로 대체할 수 있음\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x): #데이터가 지나갈 길\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2)) #두번째 입력(2)는 커널의 크기. nn.MaxPool2d 같은 일반 모듈 사용가능\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320) #2차원 형태의 x를 1차원으로 차원축소 (-1:'남는 차원 모두:전체/320', 320:'x가 가진 원소 개수')\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터 \n",
    "\n",
    "`to()` 함수는 모델의 파라미터들을 지정한 곳으로 보내는 역할을 합니다. 일반적으로 CPU 1개만 사용할 경우 필요는 없지만, GPU를 사용하고자 하는 경우 `to(\"cuda\")`로 지정하여 GPU로 보내야 합니다. 지정하지 않을 경우 계속 CPU에 남아 있게 되며 빠른 훈련의 이점을 누리실 수 없습니다.\n",
    "\n",
    "최적화 알고리즘으로 파이토치에 내장되어 있는 `optim.SGD`를 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model     = Net().to(DEVICE) #모델 인스턴스 만들기\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5) #최적화 함수 만들기. 모멘텀 첫등장 설명x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train() #학습 모드\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "\n",
    "            # 배치 오차를 합산\n",
    "            test_loss += F.cross_entropy(output, target,\n",
    "                                         reduction='sum').item()\n",
    "\n",
    "            # 가장 높은 값을 가진 인덱스가 바로 예측값\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코드 돌려보기\n",
    "\n",
    "자, 이제 모든 준비가 끝났습니다. 코드를 돌려서 실제로 학습이 되는지 확인해봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.311989\nTrain Epoch: 1 [12800/60000 (21%)]\tLoss: 1.204634\nTrain Epoch: 1 [25600/60000 (43%)]\tLoss: 1.098022\n\n\n4423680it [00:47, 223529.26it/s]\u001b[A\u001b[ATrain Epoch: 1 [38400/60000 (64%)]\tLoss: 0.955576\nTrain Epoch: 1 [51200/60000 (85%)]\tLoss: 0.921472\n[1] Test Loss: 0.6689, Accuracy: 72.67%\nTrain Epoch: 2 [0/60000 (0%)]\tLoss: 0.849364\nTrain Epoch: 2 [12800/60000 (21%)]\tLoss: 0.608787\nTrain Epoch: 2 [25600/60000 (43%)]\tLoss: 0.920404\nTrain Epoch: 2 [38400/60000 (64%)]\tLoss: 1.046688\nTrain Epoch: 2 [51200/60000 (85%)]\tLoss: 1.096114\n[2] Test Loss: 0.5725, Accuracy: 77.64%\nTrain Epoch: 3 [0/60000 (0%)]\tLoss: 0.699395\nTrain Epoch: 3 [12800/60000 (21%)]\tLoss: 0.815541\nTrain Epoch: 3 [25600/60000 (43%)]\tLoss: 0.742900\nTrain Epoch: 3 [38400/60000 (64%)]\tLoss: 0.739689\nTrain Epoch: 3 [51200/60000 (85%)]\tLoss: 0.816691\n[3] Test Loss: 0.5450, Accuracy: 79.18%\nTrain Epoch: 4 [0/60000 (0%)]\tLoss: 0.602790\nTrain Epoch: 4 [12800/60000 (21%)]\tLoss: 0.920416\nTrain Epoch: 4 [25600/60000 (43%)]\tLoss: 0.594789\nTrain Epoch: 4 [38400/60000 (64%)]\tLoss: 0.657672\nTrain Epoch: 4 [51200/60000 (85%)]\tLoss: 0.674523\n[4] Test Loss: 0.5032, Accuracy: 81.05%\nTrain Epoch: 5 [0/60000 (0%)]\tLoss: 0.647115\nTrain Epoch: 5 [12800/60000 (21%)]\tLoss: 0.627859\nTrain Epoch: 5 [25600/60000 (43%)]\tLoss: 0.542785\nTrain Epoch: 5 [38400/60000 (64%)]\tLoss: 0.664610\nTrain Epoch: 5 [51200/60000 (85%)]\tLoss: 0.614556\n[5] Test Loss: 0.4698, Accuracy: 82.07%\nTrain Epoch: 6 [0/60000 (0%)]\tLoss: 0.538297\nTrain Epoch: 6 [12800/60000 (21%)]\tLoss: 0.421409\nTrain Epoch: 6 [25600/60000 (43%)]\tLoss: 0.856920\nTrain Epoch: 6 [38400/60000 (64%)]\tLoss: 0.743875\nTrain Epoch: 6 [51200/60000 (85%)]\tLoss: 0.678887\n[6] Test Loss: 0.4428, Accuracy: 83.74%\nTrain Epoch: 7 [0/60000 (0%)]\tLoss: 0.657489\nTrain Epoch: 7 [12800/60000 (21%)]\tLoss: 0.625442\nTrain Epoch: 7 [25600/60000 (43%)]\tLoss: 0.530892\nTrain Epoch: 7 [38400/60000 (64%)]\tLoss: 0.544556\nTrain Epoch: 7 [51200/60000 (85%)]\tLoss: 0.684821\n[7] Test Loss: 0.4355, Accuracy: 83.79%\nTrain Epoch: 8 [0/60000 (0%)]\tLoss: 0.413692\nTrain Epoch: 8 [12800/60000 (21%)]\tLoss: 0.608661\nTrain Epoch: 8 [25600/60000 (43%)]\tLoss: 0.649732\nTrain Epoch: 8 [38400/60000 (64%)]\tLoss: 0.524028\nTrain Epoch: 8 [51200/60000 (85%)]\tLoss: 0.598487\n[8] Test Loss: 0.4102, Accuracy: 85.21%\nTrain Epoch: 9 [0/60000 (0%)]\tLoss: 0.553561\nTrain Epoch: 9 [12800/60000 (21%)]\tLoss: 0.532843\nTrain Epoch: 9 [25600/60000 (43%)]\tLoss: 0.427618\nTrain Epoch: 9 [38400/60000 (64%)]\tLoss: 0.535242\nTrain Epoch: 9 [51200/60000 (85%)]\tLoss: 0.388699\n[9] Test Loss: 0.3982, Accuracy: 85.27%\nTrain Epoch: 10 [0/60000 (0%)]\tLoss: 0.454315\nTrain Epoch: 10 [12800/60000 (21%)]\tLoss: 0.549101\nTrain Epoch: 10 [25600/60000 (43%)]\tLoss: 0.401490\nTrain Epoch: 10 [38400/60000 (64%)]\tLoss: 0.544054\nTrain Epoch: 10 [51200/60000 (85%)]\tLoss: 0.608608\n[10] Test Loss: 0.3926, Accuracy: 86.07%\nTrain Epoch: 11 [0/60000 (0%)]\tLoss: 0.476010\nTrain Epoch: 11 [12800/60000 (21%)]\tLoss: 0.719312\nTrain Epoch: 11 [25600/60000 (43%)]\tLoss: 0.415081\nTrain Epoch: 11 [38400/60000 (64%)]\tLoss: 0.490754\nTrain Epoch: 11 [51200/60000 (85%)]\tLoss: 0.406219\n[11] Test Loss: 0.3940, Accuracy: 85.60%\nTrain Epoch: 12 [0/60000 (0%)]\tLoss: 0.614798\nTrain Epoch: 12 [12800/60000 (21%)]\tLoss: 0.431388\nTrain Epoch: 12 [25600/60000 (43%)]\tLoss: 0.766251\nTrain Epoch: 12 [38400/60000 (64%)]\tLoss: 0.547152\nTrain Epoch: 12 [51200/60000 (85%)]\tLoss: 0.358931\n[12] Test Loss: 0.3783, Accuracy: 86.33%\nTrain Epoch: 13 [0/60000 (0%)]\tLoss: 0.285905\nTrain Epoch: 13 [12800/60000 (21%)]\tLoss: 0.446287\nTrain Epoch: 13 [25600/60000 (43%)]\tLoss: 0.483191\nTrain Epoch: 13 [38400/60000 (64%)]\tLoss: 0.559960\nTrain Epoch: 13 [51200/60000 (85%)]\tLoss: 0.535173\n[13] Test Loss: 0.3720, Accuracy: 86.07%\nTrain Epoch: 14 [0/60000 (0%)]\tLoss: 0.495526\nTrain Epoch: 14 [12800/60000 (21%)]\tLoss: 0.388237\nTrain Epoch: 14 [25600/60000 (43%)]\tLoss: 0.421368\nTrain Epoch: 14 [38400/60000 (64%)]\tLoss: 0.559809\nTrain Epoch: 14 [51200/60000 (85%)]\tLoss: 0.547800\n[14] Test Loss: 0.3715, Accuracy: 86.27%\nTrain Epoch: 15 [0/60000 (0%)]\tLoss: 0.419548\nTrain Epoch: 15 [12800/60000 (21%)]\tLoss: 0.495468\nTrain Epoch: 15 [25600/60000 (43%)]\tLoss: 0.610485\nTrain Epoch: 15 [38400/60000 (64%)]\tLoss: 0.426511\nTrain Epoch: 15 [51200/60000 (85%)]\tLoss: 0.638560\n[15] Test Loss: 0.3622, Accuracy: 86.54%\nTrain Epoch: 16 [0/60000 (0%)]\tLoss: 0.272960\nTrain Epoch: 16 [12800/60000 (21%)]\tLoss: 0.352431\nTrain Epoch: 16 [25600/60000 (43%)]\tLoss: 0.712482\nTrain Epoch: 16 [38400/60000 (64%)]\tLoss: 0.510359\nTrain Epoch: 16 [51200/60000 (85%)]\tLoss: 0.563046\n[16] Test Loss: 0.3558, Accuracy: 86.99%\nTrain Epoch: 17 [0/60000 (0%)]\tLoss: 0.511207\nTrain Epoch: 17 [12800/60000 (21%)]\tLoss: 0.518838\nTrain Epoch: 17 [25600/60000 (43%)]\tLoss: 0.362677\nTrain Epoch: 17 [38400/60000 (64%)]\tLoss: 0.534677\nTrain Epoch: 17 [51200/60000 (85%)]\tLoss: 0.438821\n[17] Test Loss: 0.3534, Accuracy: 86.75%\nTrain Epoch: 18 [0/60000 (0%)]\tLoss: 0.215167\nTrain Epoch: 18 [12800/60000 (21%)]\tLoss: 0.456473\nTrain Epoch: 18 [25600/60000 (43%)]\tLoss: 0.529941\nTrain Epoch: 18 [38400/60000 (64%)]\tLoss: 0.339720\nTrain Epoch: 18 [51200/60000 (85%)]\tLoss: 0.364959\n[18] Test Loss: 0.3504, Accuracy: 87.16%\nTrain Epoch: 19 [0/60000 (0%)]\tLoss: 0.575233\nTrain Epoch: 19 [12800/60000 (21%)]\tLoss: 0.309391\nTrain Epoch: 19 [25600/60000 (43%)]\tLoss: 0.763054\nTrain Epoch: 19 [38400/60000 (64%)]\tLoss: 0.274139\nTrain Epoch: 19 [51200/60000 (85%)]\tLoss: 0.429936\n[19] Test Loss: 0.3482, Accuracy: 87.30%\nTrain Epoch: 20 [0/60000 (0%)]\tLoss: 0.440843\nTrain Epoch: 20 [12800/60000 (21%)]\tLoss: 0.692421\nTrain Epoch: 20 [25600/60000 (43%)]\tLoss: 0.412442\nTrain Epoch: 20 [38400/60000 (64%)]\tLoss: 0.529948\nTrain Epoch: 20 [51200/60000 (85%)]\tLoss: 0.348880\n[20] Test Loss: 0.3498, Accuracy: 86.75%\nTrain Epoch: 21 [0/60000 (0%)]\tLoss: 0.406767\nTrain Epoch: 21 [12800/60000 (21%)]\tLoss: 0.413118\nTrain Epoch: 21 [25600/60000 (43%)]\tLoss: 0.532824\nTrain Epoch: 21 [38400/60000 (64%)]\tLoss: 0.508371\nTrain Epoch: 21 [51200/60000 (85%)]\tLoss: 0.423422\n[21] Test Loss: 0.3475, Accuracy: 87.41%\nTrain Epoch: 22 [0/60000 (0%)]\tLoss: 0.452499\nTrain Epoch: 22 [12800/60000 (21%)]\tLoss: 0.403713\nTrain Epoch: 22 [25600/60000 (43%)]\tLoss: 0.413104\nTrain Epoch: 22 [38400/60000 (64%)]\tLoss: 0.381317\nTrain Epoch: 22 [51200/60000 (85%)]\tLoss: 0.384611\n[22] Test Loss: 0.3386, Accuracy: 87.77%\nTrain Epoch: 23 [0/60000 (0%)]\tLoss: 0.470173\nTrain Epoch: 23 [12800/60000 (21%)]\tLoss: 0.464522\nTrain Epoch: 23 [25600/60000 (43%)]\tLoss: 0.541723\nTrain Epoch: 23 [38400/60000 (64%)]\tLoss: 0.612282\nTrain Epoch: 23 [51200/60000 (85%)]\tLoss: 0.390014\n[23] Test Loss: 0.3409, Accuracy: 87.18%\nTrain Epoch: 24 [0/60000 (0%)]\tLoss: 0.579007\nTrain Epoch: 24 [12800/60000 (21%)]\tLoss: 0.373697\nTrain Epoch: 24 [25600/60000 (43%)]\tLoss: 0.414895\nTrain Epoch: 24 [38400/60000 (64%)]\tLoss: 0.342446\nTrain Epoch: 24 [51200/60000 (85%)]\tLoss: 0.408169\n[24] Test Loss: 0.3344, Accuracy: 87.60%\nTrain Epoch: 25 [0/60000 (0%)]\tLoss: 0.477865\nTrain Epoch: 25 [12800/60000 (21%)]\tLoss: 0.451809\nTrain Epoch: 25 [25600/60000 (43%)]\tLoss: 0.680682\nTrain Epoch: 25 [38400/60000 (64%)]\tLoss: 0.549544\nTrain Epoch: 25 [51200/60000 (85%)]\tLoss: 0.426900\n[25] Test Loss: 0.3382, Accuracy: 87.41%\nTrain Epoch: 26 [0/60000 (0%)]\tLoss: 0.567304\nTrain Epoch: 26 [12800/60000 (21%)]\tLoss: 0.425761\nTrain Epoch: 26 [25600/60000 (43%)]\tLoss: 0.483922\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-54b66da45091>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
      "\u001b[0;32m<ipython-input-23-d6b87e91c5dc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-1f41ddaf7f75>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#데이터가 지나갈 길\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#두번째 입력(2)는 커널의 크기. nn.MaxPool2d 같은 일반 모듈 사용가능\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m320\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#2차원 형태의 x를 1차원으로 차원축소 (-1:'남는 차원 모두', 320:'x가 가진 원소 개수')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     return torch.max_pool2d(\n\u001b[0;32m--> 576\u001b[0;31m         input, kernel_size, stride, padding, dilation, ceil_mode)\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m max_pool2d = boolean_dispatch(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    \n",
    "    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
    "          epoch, test_loss, test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}