{
 "cells": [
  {
   "source": [
    "# RNN\n",
    "- 앞서 배운 신경망 모델들은 '연달아 있는 데이터의 순서와 상호작용을 인식하여 전체 상황을 이해'하는 능력이 없다. (=시간에 대한 개념이 없다.)\n",
    "- squential data / time series data의 정보를 받아 전체 내용을 학습.\n",
    "- 정해지지 않은 길이의 배열을 읽고 설명하는 신경망.\n",
    "- 출력은 순차적 데이터의 흐름을 모두 내포한다.\n",
    "- 시계열 데이터의 정보를 하나씩 입력받을 때마다 지금까지 입력된 벡터들을 종합해 hidden vector를 만들어낸다.\n",
    "- 마지막 은닉 벡터는 배열 속 모든 벡터들의 내용을 압축한 벡터.\n",
    "- 텍스트와 자연어, 주가를 처리하는데 주로 사용된다.\n",
    "- LSTM (=long short term memory), GRU (=gated recurrent unit)로\n",
    "- 언어 모델링(=language modeling), 텍스트 감정 분석(=text sentiment analysis), 기계 번역(=machine translation)등에 이용.\n",
    "- 일대일(일반적인 신경망, CNN).\n",
    "- 일대다(이미지를 보고 상황을 글로 설명).\n",
    "- 다대일(감정 분석).\n",
    "- 다대다1(입력이 끝나고 출력, 챗봇, 기계번역).\n",
    "- 다대다2(입력되자마자 출력, 매 프레임을 레이블링하는 비디오 분류)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프로젝트 1. 영화 리뷰 감정 분석\n",
    "**RNN 을 이용해 IMDB 데이터를 가지고 텍스트 감정분석을 해 봅시다.**\n",
    "\n",
    "이번 책에서 처음으로 접하는 텍스트 형태의 데이터셋인 IMDB 데이터셋은 50,000건의 영화 리뷰로 이루어져 있습니다.\n",
    "각 리뷰는 다수의 영어 문장들로 이루어져 있으며, 평점이 7점 이상의 긍정적인 영화 리뷰는 2로, 평점이 4점 이하인 부정적인 영화 리뷰는 1로 레이블링 되어 있습니다. 영화 리뷰 텍스트를 RNN 에 입력시켜 영화평의 전체 내용을 압축하고, 이렇게 압축된 리뷰가 긍정적인지 부정적인지 판단해주는 간단한 분류 모델을 만드는 것이 이번 프로젝트의 목표입니다.\n",
    "- 자연어 텍스트는 인공 신경망에 입력시키기 위해 전처리 과정을 거쳐 숫자로 나타내야한다.\n",
    "- 영화 리뷰를 '언어의 최소 단위'인 토큰으로 나누기.\n",
    "- 간단한 데이터셋이라면 파이썬의 split() 사용가능.\n",
    "- 더 좋은 성능을 위해서는 SpaCy 추천.\n",
    "### 문장 속 모든 토큰을 벡터로 나타내기\n",
    "1. 데이터셋의 모든 단어(토큰) 수만큼의 벡터들을 담는 dictionary 정의.\n",
    "2. 토큰을 벡터 형태로 변환(워드 임베딩)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data, datasets\n",
    "# 토치 텍스트의 전처리 도구들과 파이토치의 nn.Embedding으로 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "다음 기기로 학습합니다: cpu\n"
    }
   ],
   "source": [
    "# 하이퍼파라미터\n",
    "BATCH_SIZE = 64\n",
    "lr = 0.001\n",
    "EPOCHS = 10\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"다음 기기로 학습합니다:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "데이터 로딩중...\ndownloading aclImdb_v1.tar.gz\naclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [01:25&lt;00:00, 978kB/s]\n"
    }
   ],
   "source": [
    "# 데이터 로딩하기\n",
    "print(\"데이터 로딩중...\")\n",
    "\n",
    "# 텍스트와 라벨을 텐서로 바꿔줄 때 필요한 설정 정하기. \n",
    "# sequential : 순차적인 데이터셋인지 명시\n",
    "# batch_first : 신경망에 입력되는 텐서의 첫 번째 차원값이 batch_size가 되도록 정함.\n",
    "# lower : 텍스트의 모든 알파벳이 소문자가 되도록 처리.\n",
    "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
    "LABEL = data.Field(sequential=False, batch_first=True)\n",
    "\n",
    "# 모델에 입력되는 데이터셋 만들기.\n",
    "trainset, testset = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "# 워드 임베딩에 필요한 단어 사전(word vocabulary) 만들기.\n",
    "# min_freq=5 : 데이터에서 최소 5번 이상 등장한 단어만을 사전에 담겠다. 5번 미만으로 출현하는 단어는 unk(=Unknown)토큰으로 대체됨.\n",
    "TEXT.build_vocab(trainset, min_freq=5)\n",
    "LABEL.build_vocab(trainset)\n",
    "\n",
    "# 학습용 데이터를 학습셋 80% 검증셋 20% 로 나누기. 검증셋은 학습의 진행도를 확인하는데 사용.\n",
    "trainset, valset = trainset.split(split_ratio=0.8)\n",
    "# 반복 때마다 batch를 생성해주는 반복자iterator를 만들기. 이 반복자를 enumerate() 함수에 입력시켜 루프 구현.\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "        (trainset, valset, testset), batch_size=BATCH_SIZE,\n",
    "        shuffle=True, repeat=False)\n",
    "\n",
    "# 사전 속 단어들의 개수와 레이블의 수를 정해주는 변수 만들기.\n",
    "vocab_size = len(TEXT.vocab)\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[학습셋]: 20000 [검증셋]: 5000 [테스트셋]: 25000 [단어수]: 46159 [클래스] 2\n"
    }
   ],
   "source": [
    "print(\"[학습셋]: %d [검증셋]: %d [테스트셋]: %d [단어수]: %d [클래스] %d\"\n",
    "      % (len(trainset),len(valset), len(testset), vocab_size, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicGRU(nn.Module):\n",
    "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n",
    "        super(BasicGRU, self).__init__()\n",
    "        print(\"Building Basic GRU model...\")\n",
    "        # 은닉 벡터들의 층. 아주 복잡한 모델이 아닌 이상 2이하로 정의하는게 일반적. 여러 층이면 multilayer형태의 RNN.\n",
    "        self.n_layers = n_layers \n",
    "        # 첫번째 파라미터 : 전체 데이터셋의 사전에 등재된 단어의 수. \n",
    "        # 두번째 파라미터 : 임베딩된 단어 텐서가 지니는 차원값.\n",
    "        self.embed = nn.Embedding(n_vocab, embed_dim) \n",
    "        # 은닉벡터의 차원값.\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # 드롭아웃.\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        # 본격적인 RNN 모델 정의.\n",
    "        # 기본적인 RNN은 앞부분의 정보가 소실되기 때문에, \n",
    "        # GRU로 시계열 데이터 속 벡터 사이의 정보 전달량을 조절하여 기울기를 적정하게 유지, 앞부분의 정보가 끝까지 도달할 수 있도록 도움.\n",
    "        # Update 게이트 : 이전 은닉 벡터가 지닌 정보를 새로운 은닉 벡터가 얼마나 유지할지 정함.\n",
    "        # Reset 게이트 : 새로운 입력이 이전 은닉 벡터와 어떻게 조합하는지 결정.\n",
    "        self.gru = nn.GRU(embed_dim, self.hidden_dim,\n",
    "                          num_layers=self.n_layers,\n",
    "                          batch_first=True)\n",
    "        #시계열 데이터를 압축한 하나의 텐서를 신경망에 통과시켜 클래스에 대한 예측을 출력.\n",
    "        self.out = nn.Linear(self.hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) # 한 배치 속에 있는 영화평들을 워드 임베딩.\n",
    "        h_0 = self._init_state(batch_size=x.size(0)) #첫번째 은닉 벡터를 정의. 직접 정의한 함수를 사용. 보통 모든 특성값이 0인 벡터로 설정.\n",
    "        # gru에 입력하면 은닉 벡터들이 시계열 배열 형태로 반환됨.\n",
    "        # 결과값은 (batch_size, 입력 x의 길이, hidden_dim)의 모양을 지닌 3d 텐서.\n",
    "        x, _ = self.gru(x, h_0)  # [i, b, h]\n",
    "        h_t = x[:,-1,:] # 인덱싱으로 배치 내 모든 시계열 은닉 벡터들의 마지막 토큰을 내포한 (bach_size, 1, hidden_dim)모양의 텐서를 추출할 수 있음.\n",
    "        # h_t가 곧 영화 리뷰 배열들을 압축한 은닉 벡터\n",
    "        self.dropout(h_t) # 드롭아웃 설정\n",
    "        logit = self.out(h_t)  # [b, h] -> [b, o]\n",
    "        return logit\n",
    "    \n",
    "    def _init_state(self, batch_size=1):\n",
    "        #parameters() 함수는 nn.Module의 가중치 정보들을 iterator 형태로 반환.\n",
    "        # 이 반복자가 생성하는 원소들은 각각 실제 신경망의 가중치 텐서(.data)를 지닌 객체들임.\n",
    "        weight = next(self.parameters()).data \n",
    "        # new()함수로 모델의 가중치와 같은 모양으로 변환한 후 zero_()로 모든 값을 0으로 초기화함.\n",
    "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_iter):\n",
    "    model.train()\n",
    "    for b, batch in enumerate(train_iter):\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "        y.data.sub_(1)  # 레이블 값을 '1과 2' 에서 '0과 1'로 변환.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logit = model(x) # 예측값.\n",
    "        loss = F.cross_entropy(logit, y) #오차\n",
    "        loss.backward() # 기울기\n",
    "        optimizer.step() # 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_iter): # 검증셋과 테스트셋 성능 측정\n",
    "    \"\"\"evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    corrects, total_loss = 0, 0 \n",
    "    for batch in val_iter:\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "        y.data.sub_(1) # 레이블 값을 0과 1로 변환\n",
    "\n",
    "        logit = model(x)\n",
    "        #업데이트 없음.\n",
    "\n",
    "        loss = F.cross_entropy(logit, y, reduction='sum') # 오찻값과 정확도를 구하기 위해 배치의 평균이 아닌 합을 구함.\n",
    "        total_loss += loss.item()\n",
    "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum() # 정답 맞춘 것들 합.\n",
    "    size = len(val_iter.dataset)\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    return avg_loss, avg_accuracy # 오찻값과 정확도의 평균."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Building Basic GRU model...\n"
    }
   ],
   "source": [
    "model = BasicGRU(1, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE) # 모델 객체 정의. 은닉 벡터는 256차원. 임베딩된 토큰은 128차원.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr) # Adam 최적화 알고리즘 사용.\n",
    "\"\"\"\n",
    "Building Basic GRU model...\n",
    "BasicGRU(\n",
    "  (embed): Embedding(46159, 128)\n",
    "  (dropout): Dropout(p=0.5)\n",
    "  (gru): GRU(128, 256, batch_first=True)\n",
    "  (out): Linear(in_features=256, out_features=2, bias=True)\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m&lt;ipython-input-9-c2a4e3987abb&gt;\u001b[0m in \u001b[0;36m&lt;module&gt;\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbest_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----&gt; 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m&lt;ipython-input-6-f7c1dbbba175&gt;\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_iter)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 예측값.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#오차\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---&gt; 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 기울기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 최적화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         &quot;&quot;&quot;\n\u001b[0;32m--&gt; 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--&gt; 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "for e in range(1, EPOCHS+1):\n",
    "    train(model, optimizer, train_iter)\n",
    "    val_loss, val_accuracy = evaluate(model, val_iter)\n",
    "\n",
    "    print(\"[이폭: %d] 검증 오차:%5.2f | 검증 정확도:%5.2f\" % (e, val_loss, val_accuracy))\n",
    "    \n",
    "    # 검증 오차가 가장 적은 최적의 모델을 저장\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        if not os.path.isdir(\"snapshot\"):\n",
    "            os.makedirs(\"snapshot\") # 없으면 만들기\n",
    "        torch.save(model.state_dict(), './snapshot/txtclassification.pt') # 저장\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 오차:  0.31 | 테스트 정확도: 86.00\n"
     ]
    }
   ],
   "source": [
    "#테스트 셋으로 시험.\n",
    "model.load_state_dict(torch.load('./snapshot/txtclassification.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iter)\n",
    "print('테스트 오차: %5.2f | 테스트 정확도: %5.2f' % (test_loss, test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}